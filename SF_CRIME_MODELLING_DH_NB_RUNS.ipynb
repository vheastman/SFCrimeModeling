{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DCRH\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\DCRH\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn import metrics\n",
    "\n",
    "# true division for integers in 2.7\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(877982, 8)\n"
     ]
    }
   ],
   "source": [
    "# import training dataset, drop unwanted columns, and exclude rows with location data outside range\n",
    "\n",
    "df = pd.read_csv('train.csv', parse_dates=[0])\n",
    "df = df.drop(['Descript', 'Resolution'], axis=1)\n",
    "df = df[df.X < -121]\n",
    "df = df[df.Y < 80]\n",
    "df.Category = df.Category.astype('category')\n",
    "df['Cat_codes'] = df.Category.cat.codes\n",
    "df_train = df\n",
    "print df_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(884262, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import test data - parsing dates in column 1\n",
    "df_test = pd.read_csv('test.csv', parse_dates=[1])\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Campus Name', u'CCSF Entity', u'Lower Grade', u'Upper Grade',\n",
       "       u'Grade Range', u'Category', u'Map Label', u'Lower Age', u'Upper Age',\n",
       "       u'General Type', u'CDS Code', u'Campus Address', u'Supervisor District',\n",
       "       u'County FIPS', u'County Name', u'Location 1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import SF school data\n",
    "df_sch = pd.read_csv('schools.csv')\n",
    "df_sch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23191, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe of unique addresses\n",
    "\n",
    "df_addr = df.drop_duplicates(subset = 'Address')\n",
    "df_addr = df_addr[['Address', 'X', 'Y']]\n",
    "df_addr = df_addr.reset_index(drop=True)\n",
    "df_addr.head()\n",
    "print df_addr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column to training data that adds True / False proximity to school based on distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip X Y data from school dataset\n",
    "def school_proximity(dist, df):\n",
    "    \"\"\"add column to training data based on proximity to school\"\"\"\n",
    "    df_sch = pd.read_csv('schools.csv')\n",
    "    df_sch['Y'] = df_sch['Location 1'].str.findall('\\d\\d\\.\\d+').str.get(0).astype('float64')\n",
    "    df_sch['X'] = df_sch['Location 1'].str.findall('\\-\\d{3}\\.\\d+').str.get(0).astype('float64')\n",
    "    df_sch = df_sch[['Campus Name','X','Y']]\n",
    "\n",
    "    # create list of closest distance of each address to any school \n",
    "    closest_dist = []\n",
    "\n",
    "    for i in range(len(df_addr)):\n",
    "        df_sch['sch_dist'] = ((((df_addr.X[i]-df_sch.X)*88000)**2) + ((df_addr.Y[i]-df_sch.Y)*111000)**2)**0.5\n",
    "        closest_dist.append(min(df_sch.sch_dist))\n",
    "    \n",
    "    # add closest distance to df_addr dataframe\n",
    "    df_addr['closest_dist'] = closest_dist\n",
    "    #print df_addr.head()\n",
    "\n",
    "    # filter based on 100m distance\n",
    "    df_addr['by_school'] = df_addr.closest_dist < dist\n",
    "\n",
    "    # create dataframe with addresses and true false proximity to school\n",
    "    newgroup = df_addr[['Address','by_school']]\n",
    "    newgroup[newgroup.by_school == True]\n",
    "    \n",
    "    # create a map and map to df\n",
    "    mapper = newgroup.set_index('Address')['by_school']\n",
    "    df['by_school'] = df['Address'].map(mapper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create grouping of addresses based on volume of crime at particular address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_vol(num_group, df):\n",
    "    \"\"\"Create groups of addresses based on total volume of crime\"\"\"\n",
    "    df_addr_vol = df_train.Category.groupby(df_train.Address).value_counts().unstack(level=0).fillna(value=0).T\n",
    "    x = df_addr_vol.values\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df_addr_vol_sc = pd.DataFrame(x_scaled)\n",
    "    #print df_addr_vol_sc.shape\n",
    "    df_addr_vol_sc.head(6)\n",
    "    \n",
    "    agclv = AgglomerativeClustering(n_clusters=num_group)\n",
    "    addr_grp_vol = agclv.fit_predict(df_addr_vol_sc)\n",
    "    \n",
    "    df_addr_vol['Addr_Group_V'] = addr_grp_vol\n",
    "    df_addr_vol['Address'] = df_addr_vol.index\n",
    "    newgroupv = df_addr_vol[['Address','Addr_Group_V']]\n",
    "    \n",
    "    # create a map and map to df\n",
    "    mapper = newgroupv.set_index('Address')['Addr_Group_V']\n",
    "    df['Addr_Group_V'] = df['Address'].map(mapper)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create grouping of addresses based on ratio of crime type at particular address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_ratio(num_group, df):\n",
    "    \"\"\"Group addresses by volume of crime\"\"\"\n",
    "    df_addr_ratio = df_train.Category.groupby(df_train.Address).value_counts().unstack(level=0).fillna(value=0).T\n",
    "    df_addr_ratio = (df_addr_ratio.T / df_addr_ratio.T.sum()).T\n",
    "    df_addr_ratio.head(6)\n",
    "    \n",
    "    agclr = AgglomerativeClustering(n_clusters = num_group)\n",
    "    addr_grp_ratio = agclr.fit_predict(df_addr_ratio)\n",
    "    \n",
    "    df_addr_ratio['Addr_Group_R'] = addr_grp_ratio\n",
    "    df_addr_ratio['Address'] = df_addr_ratio.index\n",
    "    newgroupr = df_addr_ratio[['Address','Addr_Group_R']]\n",
    "    \n",
    "    # create a mapper and map to df\n",
    "    mapper = newgroupr.set_index('Address')['Addr_Group_R']\n",
    "    df['Addr_Group_R'] = df['Address'].map(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data to include address categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(sdist, vnum, rnum, df):\n",
    "    \"\"\"Preprocess dataframe\"\"\"\n",
    "    school_proximity(sdist, df)\n",
    "    group_by_vol(vnum, df)\n",
    "    group_by_ratio(rnum, df)\n",
    "    \n",
    "    # Date related information\n",
    "    df['YEAR'] = df.Dates.dt.year\n",
    "    df['MONTH'] = df.Dates.dt.month\n",
    "    df['DOM'] = df.Dates.dt.day\n",
    "    df['DOW'] = df.Dates.dt.weekday\n",
    "    df['HOUR'] = df.Dates.dt.hour\n",
    "    df['MIN'] = df.Dates.dt.minute\n",
    "    df['MIN_split'] = ((df.MIN == 0) | (df.MIN == 30))\n",
    "    df['HOUR_RATIO'] = ((df.HOUR) == 12 | (df.HOUR == 18))\n",
    "    \n",
    "    # Address related information\n",
    "    df['Block_split'] = df.Address.str.contains('Block')\n",
    "    df['X_norm'] = (df.X-df.X.mean()) / (df.X.max() - df.X.min())\n",
    "    df['Y_norm'] = (df.Y-df.Y.mean()) / (df.Y.max() - df.Y.min())\n",
    "    \n",
    "    # Combined Date and Address information\n",
    "    df['Year_District'] = df.YEAR.astype(str).str.cat(df.PdDistrict.astype(str), sep=' ')\n",
    "    \n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble preprocessed data into dataframe for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble(df):\n",
    "    \"\"\"Create dataframe suitable for machine learning\"\"\"\n",
    "    \n",
    "    # Binarize information where necessary\n",
    "    df_YEAR_DISTRICT = pd.get_dummies(df.Year_District, prefix = 'Year_District')\n",
    "    df_YEAR = pd.get_dummies(df.YEAR, prefix = 'Month')\n",
    "    df_MONTH = pd.get_dummies(df.MONTH, prefix = 'Month')\n",
    "    df_DOM = pd.get_dummies(df.DOM, prefix = 'DofM')\n",
    "    df_DOW = pd.get_dummies(df.DOW, prefix = 'DofW')\n",
    "    df_HOUR = pd.get_dummies(df.HOUR, prefix = 'Hour')\n",
    "    df_DISTRICT = pd.get_dummies(df.PdDistrict, prefix = 'District')\n",
    "    df_ADDR_GP_V = pd.get_dummies(df.Addr_Group_V, prefix = 'AddrV')\n",
    "    df_ADDR_GP_R = pd.get_dummies(df.Addr_Group_R, prefix = 'AddrR')\n",
    "    \n",
    "    # Concatenate specific fields into dataframe\n",
    "    new = pd.DataFrame(data=None, index = df.index)\n",
    "    \n",
    "    # Date Fields\n",
    "    #new = pd.concat([new, df_YEAR], axis = 1)\n",
    "    new = pd.concat([new, df_MONTH], axis = 1)\n",
    "    #new = pd.concat([new, df_DOM], axis = 1)\n",
    "    #new = pd.concat([new, df_DOW], axis = 1)\n",
    "    #new = pd.concat([new, df_HOUR], axis = 1)\n",
    "    new = pd.concat([new, df.HOUR_RATIO], axis = 1)\n",
    "    new = pd.concat([new, df.MIN_split], axis = 1)    \n",
    "   \n",
    "    # Address Fields\n",
    "    #new = pd.concat([new, df.Y_norm, df.X_norm], axis = 1)\n",
    "    #new = pd.concat([new, df_ADDR_GP_V], axis = 1)\n",
    "    #new = pd.concat([new, df_DISTRICT], axis = 1)\n",
    "    new = pd.concat([new, df.by_school], axis = 1)\n",
    "    new = pd.concat([new, df_ADDR_GP_R], axis = 1)\n",
    "    new = pd.concat([new, df_ADDR_GP_V], axis = 1)\n",
    "    new = pd.concat([new, df.Block_split], axis = 1)\n",
    "   \n",
    "    # Combination Fields\n",
    "    #new = pd.concat([new, df_YEAR_DISTRICT], axis = 1)\n",
    "    \n",
    "    new = new.fillna(0)\n",
    "    \n",
    "    print 'Full dataset shape: ', new.shape\n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_nb(df_adj): \n",
    "    nb = BernoulliNB()\n",
    "    print '\\nNB Cross Val Score', cross_val_score(nb, df_adj, df.Category, scoring = 'neg_log_loss').mean()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict(df):\n",
    "    nb = BernoulliNB()\n",
    "    nb.fit(assemble(process_data(40,3,500,df_train)), df_train.Category)\n",
    "    y_pred = nb.predict(df)\n",
    "    print y_pred[:20]\n",
    "    print y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NB model with parameters for school distance and number of groups for address by crime vol and ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape:  (877982, 519)\n"
     ]
    }
   ],
   "source": [
    "df_trainset = assemble(process_data(40,3,500,df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape:  (884262, 519)\n"
     ]
    }
   ],
   "source": [
    "df_testset = assemble(process_data(40,3,500,df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_nb(df_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape:  (877982, 519)\n",
      "['MISSING PERSON' 'OTHER OFFENSES' 'BURGLARY' 'ASSAULT' 'ASSAULT'\n",
      " 'OTHER OFFENSES' 'BURGLARY' 'VEHICLE THEFT' 'ASSAULT' 'LARCENY/THEFT'\n",
      " 'VEHICLE THEFT' 'ASSAULT' 'ASSAULT' 'LARCENY/THEFT' 'LARCENY/THEFT'\n",
      " 'OTHER OFFENSES' 'OTHER OFFENSES' 'MISSING PERSON' 'LARCENY/THEFT'\n",
      " 'VEHICLE THEFT']\n",
      "(884262L,)\n"
     ]
    }
   ],
   "source": [
    "test_predict(df_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do over / under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ros(df_adj):\n",
    "    \n",
    "    X_train, X_dev, y_train, y_dev = train_test_split(df_adj, df.Category, test_size = 0.5, stratify = df.Category, random_state = 42)\n",
    "\n",
    "    dict = df.Category.value_counts()\n",
    "    dict = np.log2(dict)\n",
    "    dict = dict / dict.sum()\n",
    "    dict = (dict*2347000)\n",
    "    dict = dict.astype(int)\n",
    "    dict = dict.to_dict()\n",
    "\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    ros = RandomOverSampler(ratio = dict, random_state = None)\n",
    "    X_resampled, y_resampled = ros.fit_sample(X_train, y_train)\n",
    "    \n",
    "    nb1 = MultinomialNB()\n",
    "    nb1.fit(X_resampled, y_resampled)\n",
    "    y_prednb = nb1.predict(X_dev)\n",
    "    print 'Oversampled NB Score\\n', metrics.classification_report(y_dev, y_prednb)\n",
    "    print 'Oversampled confusion matrix', metrics.confusion_matrix(y_dev, y_prednb)\n",
    "    \n",
    "    \n",
    "    nb2 = MultinomialNB()\n",
    "    nb2.fit(X_train, y_train)\n",
    "    y_prednb2 = nb2.predict(X_dev)\n",
    "    print 'Standard NB Score\\n', metrics.classification_report(y_dev, y_prednb2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros(assemble(pro_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "print '\\nRF Cross Val Score', cross_val_score(rf, assemble(pro_df), df.Category, scoring = 'neg_log_loss').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

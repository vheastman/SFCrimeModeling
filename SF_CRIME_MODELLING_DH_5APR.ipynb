{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn import metrics\n",
    "\n",
    "# true division for integers in 2.7\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(877982, 8)\n",
      "                Dates        Category  DayOfWeek PdDistrict  \\\n",
      "0 2015-05-13 23:53:00        WARRANTS  Wednesday   NORTHERN   \n",
      "1 2015-05-13 23:53:00  OTHER OFFENSES  Wednesday   NORTHERN   \n",
      "2 2015-05-13 23:33:00  OTHER OFFENSES  Wednesday   NORTHERN   \n",
      "3 2015-05-13 23:30:00   LARCENY/THEFT  Wednesday   NORTHERN   \n",
      "4 2015-05-13 23:30:00   LARCENY/THEFT  Wednesday       PARK   \n",
      "\n",
      "                     Address           X          Y  Cat_codes  \n",
      "0         OAK ST / LAGUNA ST -122.425892  37.774599         37  \n",
      "1         OAK ST / LAGUNA ST -122.425892  37.774599         21  \n",
      "2  VANNESS AV / GREENWICH ST -122.424363  37.800414         21  \n",
      "3   1500 Block of LOMBARD ST -122.426995  37.800873         16  \n",
      "4  100 Block of BRODERICK ST -122.438738  37.771541         16  \n"
     ]
    }
   ],
   "source": [
    "# import training dataset, drop unwanted columns, and exclude rows with location data outside range\n",
    "\n",
    "df = pd.read_csv('train.csv', parse_dates=[0])\n",
    "df = df.drop(['Descript', 'Resolution'], axis=1)\n",
    "df = df[df.X < -121]\n",
    "df = df[df.Y < 80]\n",
    "df.Category = df.Category.astype('category')\n",
    "df['Cat_codes'] = df.Category.cat.codes\n",
    "print df.shape\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Campus Name', u'CCSF Entity', u'Lower Grade', u'Upper Grade',\n",
       "       u'Grade Range', u'Category', u'Map Label', u'Lower Age', u'Upper Age',\n",
       "       u'General Type', u'CDS Code', u'Campus Address', u'Supervisor District',\n",
       "       u'County FIPS', u'County Name', u'Location 1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import SF school data\n",
    "df_sch = pd.read_csv('schools.csv')\n",
    "df_sch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23191, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe of unique addresses\n",
    "\n",
    "df_addr = df.drop_duplicates(subset = 'Address')\n",
    "df_addr = df_addr[['Address', 'X', 'Y']]\n",
    "df_addr = df_addr.reset_index(drop=True)\n",
    "df_addr.head()\n",
    "print df_addr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column to training data that adds True / False proximity to school based on distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip X Y data from school dataset\n",
    "def school_proximity(dist):\n",
    "    \"\"\"add column to training data based on proximity to school\"\"\"\n",
    "    df_sch = pd.read_csv('schools.csv')\n",
    "    df_sch['Y'] = df_sch['Location 1'].str.findall('\\d\\d\\.\\d+').str.get(0).astype('float64')\n",
    "    df_sch['X'] = df_sch['Location 1'].str.findall('\\-\\d{3}\\.\\d+').str.get(0).astype('float64')\n",
    "    df_sch = df_sch[['Campus Name','X','Y']]\n",
    "\n",
    "    # create list of closest distance of each address to any school \n",
    "    closest_dist = []\n",
    "\n",
    "    for i in range(len(df_addr)):\n",
    "        df_sch['sch_dist'] = ((((df_addr.X[i]-df_sch.X)*88000)**2) + ((df_addr.Y[i]-df_sch.Y)*111000)**2)**0.5\n",
    "        closest_dist.append(min(df_sch.sch_dist))\n",
    "    \n",
    "    # add closest distance to df_addr dataframe\n",
    "    df_addr['closest_dist'] = closest_dist\n",
    "    #print df_addr.head()\n",
    "\n",
    "    # filter based on 100m distance\n",
    "    df_addr['by_school'] = df_addr.closest_dist < dist\n",
    "\n",
    "    # create dataframe with addresses and true false proximity to school\n",
    "    newgroup = df_addr[['Address','by_school']]\n",
    "    newgroup[newgroup.by_school == True]\n",
    "    \n",
    "    # create a map and map to df\n",
    "    mapper = newgroup.set_index('Address')['by_school']\n",
    "    df['by_school'] = df['Address'].map(mapper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create grouping of addresses based on volume of crime at particular address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_vol(num_group):\n",
    "    \"\"\"Create groups of addresses based on total volume of crime\"\"\"\n",
    "    df_addr_vol = df.Category.groupby(df.Address).value_counts().unstack(level=0).fillna(value=0).T\n",
    "    x = df_addr_vol.values\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df_addr_vol_sc = pd.DataFrame(x_scaled)\n",
    "    #print df_addr_vol_sc.shape\n",
    "    df_addr_vol_sc.head(6)\n",
    "    \n",
    "    agclv = AgglomerativeClustering(n_clusters=num_group)\n",
    "    addr_grp_vol = agclv.fit_predict(df_addr_vol_sc)\n",
    "    \n",
    "    df_addr_vol['Addr_Group_V'] = addr_grp_vol\n",
    "    df_addr_vol['Address'] = df_addr_vol.index\n",
    "    newgroupv = df_addr_vol[['Address','Addr_Group_V']]\n",
    "    \n",
    "    # create a map and map to df\n",
    "    mapper = newgroupv.set_index('Address')['Addr_Group_V']\n",
    "    df['Addr_Group_V'] = df['Address'].map(mapper)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create grouping of addresses based on ratio of crime type at particular address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_ratio(num_group):\n",
    "    \"\"\"Group addresses by volume of crime\"\"\"\n",
    "    df_addr_ratio = df.Category.groupby(df.Address).value_counts().unstack(level=0).fillna(value=0).T\n",
    "    df_addr_ratio = (df_addr_ratio.T / df_addr_ratio.T.sum()).T\n",
    "    df_addr_ratio.head(6)\n",
    "    \n",
    "    agclr = AgglomerativeClustering(n_clusters = num_group)\n",
    "    addr_grp_ratio = agclr.fit_predict(df_addr_ratio)\n",
    "    \n",
    "    df_addr_ratio['Addr_Group_R'] = addr_grp_ratio\n",
    "    df_addr_ratio['Address'] = df_addr_ratio.index\n",
    "    newgroupr = df_addr_ratio[['Address','Addr_Group_R']]\n",
    "    \n",
    "    # create a mapper and map to df\n",
    "    mapper = newgroupr.set_index('Address')['Addr_Group_R']\n",
    "    df['Addr_Group_R'] = df['Address'].map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(sdist, vnum, rnum, df):\n",
    "    \"\"\"Preprocess dataframe\"\"\"\n",
    "    school_proximity(sdist)\n",
    "    group_by_vol(vnum)\n",
    "    group_by_ratio(rnum)\n",
    "    \n",
    "    # Date related information\n",
    "    df['YEAR'] = df.Dates.dt.year\n",
    "    df['MONTH'] = df.Dates.dt.month\n",
    "    df['DOM'] = df.Dates.dt.day\n",
    "    df['DOW'] = df.Dates.dt.weekday\n",
    "    df['HOUR'] = df.Dates.dt.hour\n",
    "    df['MIN'] = df.Dates.dt.minute\n",
    "    df['MIN_split'] = ((df.MIN == 0) | (df.MIN == 30))\n",
    "    df['HOUR_RATIO'] = ((df.HOUR) == 12 | (df.HOUR == 18))\n",
    "    \n",
    "    # Address related information\n",
    "    df['Block_split'] = df.Address.str.contains('Block')\n",
    "    df['X_norm'] = (df.X-df.X.mean()) / (df.X.max() - df.X.min())\n",
    "    df['Y_norm'] = (df.Y-df.Y.mean()) / (df.Y.max() - df.Y.min())\n",
    "    \n",
    "    # Combined Date and Address information\n",
    "    df['Year_District'] = df.YEAR.astype(str).str.cat(df.PdDistrict.astype(str), sep=' ')\n",
    "    \n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble preprocessed data into dataframe for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble(df):\n",
    "    \"\"\"Create dataframe suitable for machine learning\"\"\"\n",
    "    \n",
    "    # Binarize information where necessary\n",
    "    df_YEAR_DISTRICT = pd.get_dummies(df.Year_District, prefix = 'Year_District')\n",
    "    df_YEAR = pd.get_dummies(df.YEAR, prefix = 'Month')\n",
    "    df_MONTH = pd.get_dummies(df.MONTH, prefix = 'Month')\n",
    "    df_DOM = pd.get_dummies(df.DOM, prefix = 'DofM')\n",
    "    df_DOW = pd.get_dummies(df.DOW, prefix = 'DofW')\n",
    "    df_HOUR = pd.get_dummies(df.HOUR, prefix = 'Hour')\n",
    "    df_DISTRICT = pd.get_dummies(df.PdDistrict, prefix = 'District')\n",
    "    df_ADDR_GP_V = pd.get_dummies(df.Addr_Group_V, prefix = 'AddrV')\n",
    "    df_ADDR_GP_R = pd.get_dummies(df.Addr_Group_R, prefix = 'AddrR')\n",
    "    \n",
    "    # Concatenate specific fields into dataframe\n",
    "    new = pd.DataFrame(data=None, index = df.index)\n",
    "    \n",
    "    # Date Fields\n",
    "    #new = pd.concat([new, df_YEAR], axis = 1)\n",
    "    #new = pd.concat([new, df_MONTH], axis = 1)\n",
    "    #new = pd.concat([new, df_DOM], axis = 1)\n",
    "    #new = pd.concat([new, df_DOW], axis = 1)\n",
    "    #new = pd.concat([new, df_HOUR], axis = 1)\n",
    "    new = pd.concat([new, df.HOUR_RATIO], axis = 1)\n",
    "    new = pd.concat([new, df.MIN_split], axis = 1)    \n",
    "   \n",
    "    # Address Fields\n",
    "    #new = pd.concat([new, df.Y_norm, df.X_norm], axis = 1)\n",
    "    #new = pd.concat([new, df_ADDR_GP_V], axis = 1)\n",
    "    #new = pd.concat([new, df_DISTRICT], axis = 1)\n",
    "    new = pd.concat([new, df.by_school], axis = 1)\n",
    "    new = pd.concat([new, df_ADDR_GP_R], axis = 1)\n",
    "    new = pd.concat([new, df_ADDR_GP_V], axis = 1)\n",
    "    new = pd.concat([new, df.Block_split], axis = 1)\n",
    "   \n",
    "    # Combination Fields\n",
    "    #new = pd.concat([new, df_YEAR_DISTRICT], axis = 1)\n",
    "\n",
    "    print 'Full dataset shape: ', new.shape\n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_nb(df_adj): \n",
    "    nb = BernoulliNB()\n",
    "    print '\\nNB Cross Val Score', cross_val_score(nb, df_adj, df.Category, scoring = 'neg_log_loss').mean()\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NB model with parameters for school distance and number of groups for address by crime vol and ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape:  (877982, 507)\n",
      "\n",
      "NB Cross Val Score -2.353353105464952\n"
     ]
    }
   ],
   "source": [
    "train_test_nb(assemble((process_data(40,3,500,df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
